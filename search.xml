<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>自己搭建梯子</title>
      <link href="/2019/05/28/%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E6%A2%AF%E5%AD%90/"/>
      <url>/2019/05/28/%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E6%A2%AF%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<h4 id="1、挑选服务器："><a href="#1、挑选服务器：" class="headerlink" title="1、挑选服务器："></a><strong>1、挑选服务器：</strong></h4><p>​     一般供应商都会提供IP<a href="http://lib.csdn.net/base/softwaretest" target="_blank" rel="noopener">测试</a>延迟，或者看各VPS网站评测介绍，一般来讲亚洲服务器延迟更低，比如香港.新加坡就有很多电信或者网通直连机房，而美国VPS价格更便宜，而且带宽大些。</p><p>​    服务器类型，Xen KVM 性能要好过openvz，不过也要更贵一些，综上所述，建议选直连自己运营商的KVM虚拟技术的亚洲VPS。</p><p>下面我们用日本conoha VPS为例开始操作。</p><p>1.日本conoha支持支付宝支付，每月900日元、中文界面 （50rmb如果几个人合用的话成本还算可以。选择HOSTUS的香港25美元/年电信机房也不错）设置并记root密码点击追加建立服务器，这里我们选择centos 6.6 64位版本</p><p><img src="http://i3.tietuku.com/73939c8862f69acd.png" alt="img">2.回到服务器界面，点开网络配置，记下IP4地址</p><h4 id="2-用XshellPortable连接服务器"><a href="#2-用XshellPortable连接服务器" class="headerlink" title="2.用XshellPortable连接服务器"></a>2.用XshellPortable连接服务器</h4><p>填上IP地址点击确定</p><p>用户名为 root</p><p>密码为开通vps设置的9位密码</p><p>连接成功会显示，root@XXXXXX #<br>然后在设置里把右键改成复制剪贴板中的代码 如图，右键粘贴比较方便</p><h4 id="3-这里我们用秋水兄的SS一键安装脚本"><a href="#3-这里我们用秋水兄的SS一键安装脚本" class="headerlink" title="3.这里我们用秋水兄的SS一键安装脚本"></a>3.这里我们用秋水兄的SS一键安装脚本</h4><p>​    全程仅需3行代码，即使0基础毫无问题，依次输入代码回车即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh</span><br><span class="line"></span><br><span class="line">chmod +x shadowsocks.sh</span><br><span class="line"></span><br><span class="line">./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log</span><br></pre></td></tr></table></figure><p><img src="http://i3.tietuku.com/ae721437801a8c77.png" alt="img"></p><p>询问密码：输入t66y为端口密码</p><p><img src="http://i3.tietuku.com/0ffcb227120d76cd.png" alt="img"></p><p>默认端口8989.可以指定任意端口 ，回车后耐心等待安装</p><p><img src="http://i3.tietuku.com/5d03e29f5ba8a52b.png" alt="img"></p><p>安装完成后，脚本提示如下：</p><p><img src="http://i3.tietuku.com/ace55c8d0f0405c8.png" alt="img"></p><p>现在我们已经成功搭建了一个单用户版SS代理服务器，用其他帖子的教程连接该服务器即可<br>端口为8989 密码t66y</p><h4 id="4-多用户配置"><a href="#4-多用户配置" class="headerlink" title="4.多用户配置"></a>4.多用户配置</h4><p>删除原配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -f /etc/shadowsocks.json</span><br></pre></td></tr></table></figure><p>编辑配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/shadowsocks.json</span><br></pre></td></tr></table></figure><p><img src="http://i3.tietuku.com/0abd0889dc17cb2e.png" alt="img"></p><p>按下I键，进入编辑状态<br>左下角有标示—INSERT—<br>如图复制下面代码至配置文件<br>配置端口 8989到9004，密码t66y0到t66y4等5个账号<br>可照例加入更多用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&quot;local_address&quot;:&quot;127.0.0.1&quot;,</span><br><span class="line">&quot;local_port&quot;:1080,</span><br><span class="line">&quot;port_password&quot;:&#123;</span><br><span class="line">&quot;8989&quot;:&quot;t66y0&quot;,</span><br><span class="line">&quot;9001&quot;:&quot;t66y1&quot;,</span><br><span class="line">&quot;9002&quot;:&quot;t66y2&quot;,</span><br><span class="line">&quot;9003&quot;:&quot;t66y3&quot;,</span><br><span class="line">&quot;9004&quot;:&quot;t66y4&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&quot;timeout&quot;:300,</span><br><span class="line">&quot;method&quot;:&quot;aes-256-cfb&quot;,</span><br><span class="line">&quot;fast_open&quot;: false</span><br></pre></td></tr></table></figure><p>按下ESC键退出编辑状态，同时按下SHIFT+Q键进入退出模式<br>如图输入wq回车保存退出</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wd</span><br></pre></td></tr></table></figure><h4 id="5、重启-SS服务"><a href="#5、重启-SS服务" class="headerlink" title="5、重启 SS服务"></a>5、重启 SS服务</h4><p><code>/etc/init.d/shadowsocks restart</code></p><p>至此一个多用户版本的SS服务器已经搭建完成<br>其他命令：<br>启动：/etc/init.d/shadowsocks start<br>停止：/etc/init.d/shadowsocks stop<br>重启：/etc/init.d/shadowsocks restart<br>状态：/etc/init.d/shadowsocks status<br>卸载：./shadowsocks.sh uninstall</p><h4 id="6、Shadowsocks-WIN客户端设置"><a href="#6、Shadowsocks-WIN客户端设置" class="headerlink" title="6、Shadowsocks WIN客户端设置"></a>6、Shadowsocks WIN客户端设置</h4><p>Win客户端下载地址：<a href="http://sourceforge.NET/projects/shadowsocksgui/files/dist/" target="_blank" rel="noopener">http://sourceforge.NET/projects/shadowsocksgui/files/dist/</a></p><p>设置界面如下：</p><p>其中：Server IP为服务器IP，Server Port为远程端口（在服务器端shadowsocks.json中设置），Password为密码，Encryption为加密方式，选择<code>AES-256-CFB</code>，Proxy Port为本地端口（在服务器端shadowsocks.json中设置），Remarks为别名。</p><p>配置好客户端后，我们需要选择合适的浏览器和插件来应用本地代理，下面分别介绍了Chrome和Firefox的设置方法。</p><p><strong>a、Chrome</strong></p><p>Chrome使用本地代理需要用到插件<a href="http://switchysharp.com/install.html" target="_blank" rel="noopener">SwitchySharp</a>，安装好插件后，打开插件的设置界面，填入如下设置</p><p>设置完成后选择插件的代理模式为<code>Shadowsocks</code>(或者你自己命名的情景模式)后即可。</p><p>上面的设置为全局代理，如需实现智能代理需要手动添加规则，还可以订阅GFWlist，地址为：<a href="http://autoproxy-gfwlist.googlecode.com/svn/trunk/gfwlist.txt" target="_blank" rel="noopener">http://autoproxy-gfwlist.googlecode.com/svn/trunk/gfwlist.txt</a> 由于这个地址不通过代理无法访问，所以你可以通过其它途径下载到本地，这方面资料网上比较丰富，再者使用起来不是很方便，在此我就不赘述了。下面介绍另一种规则，<a href="https://github.com/clowwindy/" target="_blank" rel="noopener">gfwlist2pac</a>，这是网友在Gfwlist的基础上，更新了部分网址转化成的PAC规则文件，目前我就采用的是这种方式，体验不错，当然，由于规则文件都具有时效性，也许你看到这篇文章时这个规则或许不是最好用的了，这里只是讲一种思路，你可以自行选择其他规则，甚至是自定义的规则，使用PAC规则设置如下:</p><p>PAC规则地址：<a href="https://raw.githubusercontent.com/clowwindy/gfwlist2pac/master/test/proxy_abp.pac" target="_blank" rel="noopener">https://raw.githubusercontent.com/clowwindy/gfwlist2pac/master/test/proxy_abp.pac</a></p><p>设置完成后选择插件的代理模式为<code>gfwlist2pac</code>(或者你自己命名的情景模式)后即可。</p><p><strong>b、Firefox</strong></p><p>Firefox使用本地代理需要用到插件<a href="http://fxthunder.com/blog/archives/2866/" target="_blank" rel="noopener">Autoproxy</a>，这个插件原作者已经没有更新了，本文使用的是其他作者的继续更新版，修复了无法订阅gfwlist的bug，订阅方法和上述类似，同样由于原地址无法直接访问，所以可以通过其他途径下载到本地然后导入。</p><h4 id="7、Shadowsocks-Android客户端设置"><a href="#7、Shadowsocks-Android客户端设置" class="headerlink" title="7、Shadowsocks  Android客户端设置"></a>7、Shadowsocks  Android客户端设置</h4><p>首先需要下载android客户端，Shadowsocks的中文名称为影梭，可以从googleplay下载，如果你无法使用googleplay,可从下面的地址下载:<a href="https://github.com/shadowsocks/shadowsocks-android/releases" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-android/releases</a> ，android版的设置和PC端类似</p>]]></content>
      
      
      <categories>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 梯子 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络模型和tcp/ip协议</title>
      <link href="/2019/05/28/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%92%8Ctcp-ip%E5%8D%8F%E8%AE%AE/"/>
      <url>/2019/05/28/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%92%8Ctcp-ip%E5%8D%8F%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<h4 id="0、网络模型自上而下共分为七层："><a href="#0、网络模型自上而下共分为七层：" class="headerlink" title="0、网络模型自上而下共分为七层："></a>0、网络模型自上而下共分为七层：</h4><p>7 应用层<br>6 表示层<br>5 会话层<br>4 传输层<br>3 网络层<br>2 数据链路层<br>1 物理层</p><p><strong>其中3、2、1层主要面向通过网络的端到端的数据流，7、6、5、4层定义了应用程序的功能。</strong></p><p><strong>（1）应用层</strong>：与其他计算机进行通讯的一个应用，它是对应应用程序的通信服务的。例如，一个没有通信功能的字处理程序就不能执行通信的代码，从事字处理工作的程序员也不关心OSI的第7层。但是，如果添加了一个传输文件的选项，那么字处理器的程序员就需要实现OSI的第7层。示例：telnet，HTTP,FTP,WWW,NFS,SMTP等。</p><p><strong>（2）表示层</strong>：这一层的主要功能是定义数据格式及加密。例如，FTP允许你选择以二进制或ASII格式传输。如果选择二进制，那么发送方和接收方不改变文件的内容。如果选择ASII格式，发送方将把文本从发送方的字符集转换成标准的ASII后发送数据。在接收方将标准的ASII转换成接收方计算机的字符集。示例：加密，ASII等。</p><p><strong>（3）会话层</strong>：他定义了如何开始、控制和结束一个会话，包括对多个双向小时的控制和管理，以便在只完成连续消息的一部分时可以通知应用，从而使表示层看到的数据是连续的，在某些情况下，如果表示层收到了所有的数据，则用数据代表表示层。示例：RPC，SQL等。</p><p><strong>（4）传输层</strong>：这层的功能包括是否选择差错恢复协议还是无差错恢复协议，及在同一主机上对不同应用的数据流的输入进行复用，还包括对收到的顺序不对的数据包的重新排序功能。示例：TCP，UDP，SPX。</p><p><strong>（5）网络层</strong>：这层对端到端的包传输进行定义，他定义了能够标识所有结点的逻辑地址，还定义了路由实现的方式和学习的方式。为了适应最大传输单元长度小于包长度的传输介质，网络层还定义了如何将一个包分解成更小的包的分段方法。示例：IP,IPX等。</p><p><strong>（6）数据链路层</strong>：他定义了在单个链路上如何传输数据。这些协议与被讨论的歌种介质有关。示例：ATM，FDDI等。</p><p><strong>（7）物理层</strong>：OSI的物理层规范是有关传输介质的特性标准，这些规范通常也参考了其他组织制定的标准。连接头、针、针的使用、电流、电流、编码及光调制等都属于各种物理层规范中的内容。物理层常用多个规范完成对所有细节的定义。示例：Rj45，802.3等。</p><p><strong>OSI分层的优点：</strong><br>（1）人们可以很容易的讨论和学习协议的规范细节。<br>（2）层间的标准接口方便了工程模块化。<br>（3）创建了一个更好的互连环境。<br>（4）降低了复杂度，使程序更容易修改，产品开发的速度更快。<br>（5）每层利用紧邻的下层服务，更容易记住个层的功能。</p><p><strong>tcp/ip</strong></p><p>TCP/IP是“transmission Control Protocol/Internet Protocol”的简写，中文译名为传输控制协议/互联网络协议）协议。</p><p>TCP/IP（传输控制协议/网间协议）是一种网络通信协议，它规范了网络上的所有通信设备，尤其是一个主机与另一个主机之间的数据往来格式以及传送方式。TCP/IP是INTERNET的基础协议，也是一种电脑数据打包和寻址的标准方法。在数据传送中，可以形象地理解为有两个信封，TCP和IP就像是信封，要传递的信息被划分成若干段，每一段塞入一个TCP信封，并在该信封面上记录有分段号的信息，再将TCP信封塞入IP大信封，发送上网。在接受端，一个TCP软件包收集信封，抽出数据，按发送前的顺序还原，并加以校验，若发现差错，TCP将会要求重发。因此，TCP/IP在INTERNET中几乎可以无差错地传送数据。对普通用户来说，并不需要了解网络协议的整个结构，仅需了解IP的地址格式，即可与世界各地进行网络通信。<br>​        </p>]]></content>
      
      
      <categories>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络模型 </tag>
            
            <tag> tcp </tag>
            
            <tag> ip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Springboot 环境配置</title>
      <link href="/2018/12/28/Springboot-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/12/28/Springboot-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h4 id="resource中常见有几种环境配置文件"><a href="#resource中常见有几种环境配置文件" class="headerlink" title="resource中常见有几种环境配置文件"></a><strong>resource中常见有几种环境配置文件</strong></h4><p>​    application-dev.yml（开发环境）<br>     application-test.yml（测试环境）<br>     application-uat.yml（预发布）<br>     application-pro.yml（生产环境）<br>     application.yml</p><h4 id="使用多环境配置有三种方式："><a href="#使用多环境配置有三种方式：" class="headerlink" title="使用多环境配置有三种方式："></a><strong>使用多环境配置有三种方式</strong>：</h4><ol><li>使用@PropertySource注解<br>@PropertySource(classpath:application-dev.yml)</li><li><p>修改spring.profiles.active属性：</p><p>spring.profiles.active:dev</p></li><li><p>执行命令行<br>通过命令行可以直接指定某一配置文件<br>例：java -jar xxx.jar –spring.profiles.active=test</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> springboot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> springboot </tag>
            
            <tag> 环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark 数据倾斜（Data Skew）</title>
      <link href="/2018/04/28/spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%88Data-Skew%EF%BC%89/"/>
      <url>/2018/04/28/spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%88Data-Skew%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h4 id="0、前言"><a href="#0、前言" class="headerlink" title="0、前言"></a>0、前言</h4><p>​    何谓数据倾斜？数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p><p>​    数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p><p><strong>1 数据倾斜直接会导致一种情况：Out Of Memory。</strong></p><p><strong>2 运行速度慢,特别慢，非常慢，极端的慢，不可接受的慢</strong></p><p>搞定数据倾斜需要：</p><p>1 搞定shuffle</p><p>2 搞定业务场景</p><p>3 搞定 cpu core的使用情况</p><p>4 搞定OOM的根本原因等。</p><p>​    </p><p>​    一个经验结论是：一般情况下，OOM的原因都是数据倾斜。某个task任务数据量太大，GC的压力就很大。这比不了Kafka,因为kafka的内存是不经过JVM的。是基于Linux内核的Page.</p><p>​    <strong>如何定位导致数据倾斜的代码？</strong></p><p>​    数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><p>​    举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val sampledPairs = pairs.sample(false, 0.1)</span><br><span class="line"></span><br><span class="line">val sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line"></span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h4 id="1、如何缓解-消除数据倾斜？"><a href="#1、如何缓解-消除数据倾斜？" class="headerlink" title="1、如何缓解/消除数据倾斜？"></a><strong>1、如何缓解/消除数据倾斜？</strong></h4><h5 id="方式一"><a href="#方式一" class="headerlink" title="方式一"></a>方式一</h5><p><strong>尽量避免数据源的数据倾斜</strong><br>​    比如数据源是Kafka</p><p>​    以Spark Stream通过DirectStream方式读取Kafka数据为例。由于Kafka的每一个Partition对应Spark的一个Task（Partition），所以Kafka内相关Topic的各Partition之间数据是否平衡，直接决定Spark处理该数据时是否会产生数据倾斜。</p><p>​    Kafka某一Topic内消息在不同Partition之间的分布，主要由Producer端所使用的Partition实现类决定。如果使用随机Partitioner，则每条消息会随机发送到一个Partition中，从而从概率上来讲，各Partition间的数据会达到平衡。此时源Stage（直接读取Kafka数据的Stage）不会产生数据倾斜。</p><p><strong>解决方案：</strong></p><p>​    将Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能</p><p><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点</strong>：治标不治本，Hive ETL中还是会发生数据倾斜。</p><h5 id="方式二"><a href="#方式二" class="headerlink" title="方式二"></a>方式二</h5><p><strong>调整并行度分散同一个Task的不同Key</strong><br><strong>案适用场景</strong>：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路</strong>：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>原理</strong></p><p>​    Spark在做Shuffle时，默认使用HashPartitioner（非Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的Key对应的数据被分配到了同一个Task上，造成该Task所处理的数据远大于其它Task，从而造成数据倾斜。</p><p><strong>优势</strong></p><p>​    实现简单，可在需要Shuffle的操作算子上直接设置并行度或者使用spark.default.parallelism设置。如果是Spark SQL，还可通过SET spark.sql.shuffle.partitions=[num_tasks]设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。</p><p><strong>劣势</strong></p><p>​    适用场景少，只能将分配到同一Task的不同Key分散开，但对于同一Key倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p><h5 id="方式三"><a href="#方式三" class="headerlink" title="方式三"></a><strong>方式三</strong></h5><p><strong>自定义Partitioner</strong><br><strong>原理</strong></p><p>​    使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class CustomerPartitioner(numParts:Int) extends org.apache.spark.Partitioner &#123;</span><br><span class="line"></span><br><span class="line">//覆盖分区数</span><br><span class="line"></span><br><span class="line">override def numPartitions: Int = numParts</span><br><span class="line"></span><br><span class="line">//覆盖分区号获取函数</span><br><span class="line"></span><br><span class="line">override def getPartition(key: Any): Int = &#123;</span><br><span class="line"></span><br><span class="line">val id: Int = key.toString.toInt</span><br><span class="line"></span><br><span class="line">if (id = 900000)</span><br><span class="line"></span><br><span class="line">return new java.util.Random().nextInt(100) % 12</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line"></span><br><span class="line">return id % 12</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>优势</strong></p><p>​    不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p><p><strong>劣势</strong></p><p>​    适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><h5 id><a href="#" class="headerlink" title=" "></a> </h5><h5 id="方式四"><a href="#方式四" class="headerlink" title="方式四"></a>方式四</h5><p><strong>将Reduce side Join转变为Map side Join</strong><br><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><h5 id="方式五"><a href="#方式五" class="headerlink" title="方式五"></a><strong>方式五</strong></h5><p><strong>两阶段聚合（局部聚合+全局聚合）</strong><br><strong>方案适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p>​    比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><h5 id="方式六"><a href="#方式六" class="headerlink" title="方式六"></a>方式六</h5><p><strong>为数据倾斜的key增加随机前/后缀</strong><br><strong>原理</strong></p><p>​    为数据量特别大的Key增加随机前/后缀，使得原来Key相同的数据变为Key不相同的数据，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一侧的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。</p><p><strong>适用场景</strong></p><p>​    两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong></p><p>​    将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><strong>优势</strong></p><p>相对于Map则Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong></p><p>如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><h5 id="方式七"><a href="#方式七" class="headerlink" title="方式七"></a>方式七</h5><p><strong>使用随机前缀和扩容RDD进行join</strong><br><strong>方案适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p>​    将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</p><p><strong>方案优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><h5 id="方式八"><a href="#方式八" class="headerlink" title="方式八"></a>方式八</h5><p><strong>采样倾斜key并分拆join操作</strong><br><strong>方案实现思路</strong>：</p><p>​    对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</p><p>​    然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</p><p>​    接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</p><p>​    再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。而另外两个普通的RDD就照常join即可。最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p><p><strong>方案实现原理</strong>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点</strong>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点</strong>：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><h5 id="方式九"><a href="#方式九" class="headerlink" title="方式九"></a>方式九</h5><p><strong>过滤少数导致倾斜的key</strong><br><strong>方案适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 数据倾斜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sparksql处理小文件</title>
      <link href="/2017/09/28/sparksql%E5%A4%84%E7%90%86%E5%B0%8F%E6%96%87%E4%BB%B6/"/>
      <url>/2017/09/28/sparksql%E5%A4%84%E7%90%86%E5%B0%8F%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h4 id="spark处理方式一："><a href="#spark处理方式一：" class="headerlink" title="spark处理方式一："></a>spark处理方式一：</h4><p>val value: RDD[(Text, Text)] = sc.newAPIHadoopFile[Text,Text,CombineTextInputFormat](“hdfs://localhost:9000/test/hadoopkv1”)</p><h4 id="spark处理方式二："><a href="#spark处理方式二：" class="headerlink" title="spark处理方式二："></a>spark处理方式二：</h4><p>var hadoopConf = new Configuration()<br>hadoopConf.set(“mapreduce.input.fileinputformat.split.maxsize”, “512000000”)<br>hadoopConf.set(“mapreduce.input.fileinputformat.split.minsize”, “268435456”)<br>hadoopConf.set(“mapreduce.input.fileinputformat.split.minsize.per.node”, “134217728”)   //下面这两参数可以不设置，详情看文章末尾<br>hadoopConf.set(“mapreduce.input.fileinputformat.split.minsize.per.rack”, “268435456”)</p><p>val data = sc.newAPIHadoopFile(args(1),<br>  classOf[CombineTextInputFormat],<br>  classOf[LongWritable],<br>  classOf[Text], hadoopConf)</p><h4 id="hive处理小文件："><a href="#hive处理小文件：" class="headerlink" title="hive处理小文件："></a>hive处理小文件：</h4><p>sparksession.sqlContext.setConf(“hive.merge.mapfiles”,”true”)<br>sparksession.sqlContext.setConf(“mapred.max.split.size”,”256000000”)<br>sparksession.sqlContext.setConf(“mapred.min.split.size.per.node”,”192000000”)<br>sparksession.sqlContext.setConf(“mapred.min.split.size.per.rack”,”192000000”)<br>sparksession.sqlContext.setConf(“hive.input.format”,”org.apache.hadoop.hive.ql.io.CombineHiveInputFormat”)</p>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> sparksql </tag>
            
            <tag> 小文件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala 集合转换 java集合</title>
      <link href="/2017/02/28/scala-%E9%9B%86%E5%90%88%E8%BD%AC%E6%8D%A2-java%E9%9B%86%E5%90%88/"/>
      <url>/2017/02/28/scala-%E9%9B%86%E5%90%88%E8%BD%AC%E6%8D%A2-java%E9%9B%86%E5%90%88/</url>
      
        <content type="html"><![CDATA[<p>使用 scala.collection.JavaConverters 与Java集合交互。它有一系列的隐式转换，添加了asJava和asScala的转换方法。使用它们这些方法确保转换是显式的，有助于阅读：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import scala.collection.JavaConverters._</span><br><span class="line"></span><br><span class="line">val list: java.util.List[Int] = Seq(1,2,3,4).asJava</span><br><span class="line">val buffer: scala.collection.mutable.Buffer[Int] = list.asScala</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> scala </tag>
            
            <tag> 集合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark on yarn</title>
      <link href="/2017/01/28/spark-on-yarn/"/>
      <url>/2017/01/28/spark-on-yarn/</url>
      
        <content type="html"><![CDATA[<p>​    Spark在yarn集群上的部署方式分为两种，yarn client（driver运行在客户端）和yarn cluster（driver运行在master上）</p><h4 id="1、driver-on-master。"><a href="#1、driver-on-master。" class="headerlink" title="1、driver on master。"></a>1、driver on master。</h4><p>​    <strong>(1)</strong> Spark Yarn Client向YARN中提交应用程序，包括Application Master程序、启动Application Master的命令、需要在Executor中运行的程序等；</p><p>​    <strong>(2)</strong> Resource manager收到请求后，在其中一个node manager中为应用程序分配一个container，要求它在container中启动应用程序的Application Master，Application master初始化sparkContext以及创建DAG Scheduler和Task Scheduler。</p><p>​    <strong>(3)</strong> Application master根据sparkContext中的配置，向resource manager申请container，同时，Application master向Resource manager注册，这样用户可通过Resource manager查看应用程序的运行状态</p><p>​    <strong>(4)</strong> Resource manager 在集群中寻找符合条件的node manager，在node manager启动container，要求container启动executor，</p><p>​    <strong>(5)</strong> Executor启动后向Application master注册，并接收Application master分配的task</p><p>​    <strong>(6)</strong> 应用程序运行完成后，Application Master向Resource Manager申请注销并关闭自己。</p><h4 id="2、Driver-on-client"><a href="#2、Driver-on-client" class="headerlink" title="2、Driver on client"></a>2、Driver on client</h4><p>​    <strong>(1)</strong> Spark Yarn Client向YARN的Resource Manager申请启动Application Master。同时在SparkContent初始化中将创建DAG Scheduler和TASK Scheduler等</p><p>​    <strong>(2)</strong> ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派</p><p>​    <strong>(3)</strong> Client中的SparkContext初始化完毕后，与Application Master建立通讯，向Resource Manager注册，根据任务信息向Resource Manager申请资源(Container)</p><p>​    <strong>(4)</strong> 当application master申请到资源后，便与node manager通信，要求它启动container</p><p>​    <strong>(5)</strong> Container启动后向driver中的sparkContext注册，并申请task</p><p>​    <strong>(6)</strong> 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。</p><h4 id="3、两种方式的区别"><a href="#3、两种方式的区别" class="headerlink" title="3、两种方式的区别"></a>3、两种方式的区别</h4><p>​    Yarn-client和Yarn cluster模式对比可以看出，在Yarn-client（Driver on client）中，Application Master仅仅从Yarn中申请资源给Executor，之后client会跟container通信进行作业的调度。如果client离集群距离较远，建议不要采用此方式，不过此方式有利于交互式的作业。</p>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark的Shuffle过程介绍</title>
      <link href="/2017/01/28/spark%E7%9A%84Shuffle%E8%BF%87%E7%A8%8B%E4%BB%8B%E7%BB%8D/"/>
      <url>/2017/01/28/spark%E7%9A%84Shuffle%E8%BF%87%E7%A8%8B%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h4 id="HashShuffle过程介绍"><a href="#HashShuffle过程介绍" class="headerlink" title="HashShuffle过程介绍"></a>HashShuffle过程介绍</h4><p>​    Spark丰富了任务类型，有些任务之间数据流转不需要通过Shuffle，但是有些任务之间还是需要通过Shuffle来传递数据，比如wide dependency的group by key。</p><p>​    Spark中需要Shuffle输出的Map任务会为每个Reduce创建对应的bucket，Map产生的结果会根据设置的partitioner得到对应的bucketId，然后填充到相应的bucket中去。每个Map的输出结果可能包含所有的Reduce所需要的数据，所以每个Map会创建R个bucket（R是reduce的个数），M个Map总共会创建M*R个bucket。</p><p>​    Map创建的bucket其实对应磁盘上的一个文件，Map的结果写到每个bucket中其实就是写到那个磁盘文件中，这个文件也被称为blockFile，是Disk Block Manager管理器通过文件名的Hash值对应到本地目录的子目录中创建的。每个Map要在节点上创建R个磁盘文件用于结果输出，Map的结果是直接输出到磁盘文件上的，100KB的内存缓冲是用来创建Fast Buffered OutputStream输出流。这种方式一个问题就是Shuffle文件过多。</p><pre><code>每一个Mapper创建出和Reducer数目相同的bucket，bucket实际上是一个buffer，其大小为shuffle.file.buffer.kb（默认32KB）。</code></pre><p>Mapper产生的结果会根据设置的partition算法填充到每个bucket中去，然后再写入到磁盘文件。<br>Reducer从远端或是本地的block manager中找到相应的文件读取数据。</p><p>​    针对上述Shuffle过程产生的文件过多问题，Spark有另外一种改进的Shuffle过程：consolidation Shuffle，以期显著减少Shuffle文件的数量。在consolidation Shuffle中每个bucket并非对应一个文件，而是对应文件中的一个segment部分。Job的map在某个节点上第一次执行，为每个reduce创建bucket对应的输出文件，把这些文件组织成ShuffleFileGroup，当这次map执行完之后，这个ShuffleFileGroup可以释放为下次循环利用；当又有map在这个节点上执行时，不需要创建新的bucket文件，而是在上次的ShuffleFileGroup中取得已经创建的文件继续追加写一个segment；当前次map还没执行完，ShuffleFileGroup还没有释放，这时如果有新的map在这个节点上执行，无法循环利用这个ShuffleFileGroup，而是只能创建新的bucket文件组成新的ShuffleFileGroup来写输出。</p><p><strong>优点</strong></p><pre><code>1、快-不需要排序，也不需要维持hash表2、不需要额外空间用作排序3、不需要额外IO-数据写入磁盘只需一次，读取也只需一次</code></pre><p><strong>缺点</strong></p><pre><code>1、当partitions大时，输出大量的文件（cores * R）,性能开始降低2、大量的文件写入，使文件系统开始变为随机写，性能比顺序写要降低100倍3、缓存空间占用比较大</code></pre><h4 id="SortShuffle过程介绍"><a href="#SortShuffle过程介绍" class="headerlink" title="SortShuffle过程介绍"></a>SortShuffle过程介绍</h4><p>​    从1.2.0开始默认为sort shuffle(spark.shuffle.manager= sort)，实现逻辑类似于Hadoop MapReduce，Hash Shuffle每一个reducers产生一个文件，但是Sort Shuffle只是产生一个按照reducer id排序可索引的文件，这样，只需获取有关文件中的相关数据块的位置信息，并fseek就可以读取指定reducer的数据。但对于rueducer数比较少的情况，Hash Shuffle明显要比Sort Shuffle快，因此Sort Shuffle有个“fallback”计划，对于reducers数少于 “spark.shuffle.sort.bypassMergeThreshold” (200 by default)，我们使用fallback计划，hashing相关数据到分开的文件，然后合并这些文件为一个，具体实现为BypassMergeSortShuffleWriter。</p><p>​    在map进行排序，在reduce端应用Timsort[1]进行合并。map端是否容许spill，通过spark.shuffle.spill来设置，默认是true。设置为false，如果没有足够的内存来存储map的输出，那么就会导致OOM错误，因此要慎用。</p><p>​    spark使用AppendOnlyMap存储map输出的数据，利用开源hash函数MurmurHash3和平方探测法把key和value保存在相同的array中。这种保存方法可以是spark进行combine。如果spill为true，会在spill前sort。</p><p>​    与hash shuffle相比，sort shuffle中每个Mapper只产生一个数据文件和一个索引文件，数据文件中的数据按照Reducer排序，但属于同一个Reducer的数据不排序。Mapper产生的数据先放到AppendOnlyMap这个数据结构中，如果内存不够，数据则会spill到磁盘，最后合并成一个文件。<br>与Hash shuffle相比，shuffle文件数量减少，内存使用更加可控。但排序会影响速度。</p><p><strong>优点</strong></p><pre><code>1 map创建文件量较少2 少量的IO随机操作，大部分是顺序读写</code></pre><p><strong>缺点</strong></p><pre><code>1 要比Hash Shuffle要慢，需要自己通过shuffle.sort.bypassMergeThreshold来设置合适的值。2 如果使用SSD盘存储shuffle数据，那么Hash Shuffle可能更合适。</code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> shuffle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark通信架构</title>
      <link href="/2017/01/23/Spark%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/"/>
      <url>/2017/01/23/Spark%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<ul><li>Spark一开始使用 Akka 作为内部通信部件。<ul><li>在Spark 1.3年代，为了解决大块数据（如Shuffle）的传输问题，Spark引入了Netty通信框架。到了 Spark 1.6, Spark可以配置使用 Akka 或者 Netty 了，这意味着 Netty 可以完全替代 Akka了。再到 Spark 2, Spark 已经完全抛弃 Akka了，全部使用Netty了。</li></ul></li><li>为什么呢？官方的解释是：<ul><li>很多Spark用户也使用Akka，但是由于Akka不同版本之间无法互相通信，这就要求用户必须使用跟Spark完全一样的Akka版本，导致用户无法升级Akka。</li><li>Spark的Akka配置是针对Spark自身来调优的，可能跟用户自己代码中的Akka配置冲突。</li><li>Spark用的Akka特性很少，这部分特性很容易自己实现。同时，这部分代码量相比Akka来说少很多，debug比较容易。如果遇到什么bug，也可以自己马上fix，不需要等Akka上游发布新版本。而且，Spark升级Akka本身又因为第一点会强制要求用户升级他们使用的Akka，对于某些用户来说是不现实的。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 通信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GC 日志分析</title>
      <link href="/2016/09/21/GC-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"/>
      <url>/2016/09/21/GC-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>​    2017-07-20T16:32:19.836+0800: 1216.559: [GC pause (G1 Evacuation Pause) (young), 0.0797857 secs]<br>​    这是最顶层的信息，它告诉我们这是一个从进程启动后1216.559秒开始的一个疏散暂停，在这时年轻代所有的区域被疏散，如Eden和Survivor。这次收集用了0.0797857秒完成的。</p><p>[Parallel Time: 58.5 ms, GC Workers: 8]<br>并行时间是所有并行GC工作线程所花费的总时间</p><p>[GC Worker Start (ms): Min: 1216567.9, Avg: 1216568.1, Max: 1216568.4, Diff: 0.5]<br>所有工作线程的平均、最小、最大和差异时间</p><p>……</p><p>[Eden: 180.0M(180.0M)-&gt;0.0B(178.0M) Survivors: 26.0M-&gt;16.0M Heap: 432.5M(512.0M)-&gt;262.5M(512.0M)]<br>这显示了Eden占用了180M，在收集前它的容量也是180M。收集之后，它的容量降到了0，自所有对象从Eden区疏散/晋升后。它的目标大小增长到了178M,<br>Survivors收集之后Survivor从26M变到16M，<br>Heap堆空间总占有量和总容量分别是432M和512M回收之前，回收之后分别变为262和512M。</p>]]></content>
      
      
      <categories>
          
          <category> jvm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GC </tag>
            
            <tag> jvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA -Xms -Xmx -XX:PermSize -XX:MaxPermSize 区别</title>
      <link href="/2016/08/28/JAVA-Xms-Xmx-XX-PermSize-XX-MaxPermSize-%E5%8C%BA%E5%88%AB/"/>
      <url>/2016/08/28/JAVA-Xms-Xmx-XX-PermSize-XX-MaxPermSize-%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h4 id="1、参数设置背景"><a href="#1、参数设置背景" class="headerlink" title="1、参数设置背景"></a>1、参数设置背景</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在做java开发时尤其是大型软件开发时经常会遇到内存溢出的问题，比如说OutOfMemoryError等。这是个让开发人员很痛苦、也很纠结的问题，因为我们有时不知道什么样的操作导致了这种问题的发生。所以我们不得不通过不断的审查、优化自己的代码结构。但是有时我们会发现有些时候不单单是通过重构自身的代码就能够解决这样的问题，因为也可能是由于我们对java虚拟机运行时的内存分配的不得当导致了内存溢出现象的不断发生。</span><br><span class="line"></span><br><span class="line">为了解决这一问题，java开发团队提供了一个用户自定义的方式按需配置java虚拟机运行时的所需的内存——通过参数配置的形式实现参数分配自定义化。</span><br></pre></td></tr></table></figure><h4 id="2、JVM按照其存储数据的内容将所需内存分配为堆区与非堆区两个部分："><a href="#2、JVM按照其存储数据的内容将所需内存分配为堆区与非堆区两个部分：" class="headerlink" title="2、JVM按照其存储数据的内容将所需内存分配为堆区与非堆区两个部分："></a>2、JVM按照其存储数据的内容将所需内存分配为堆区与非堆区两个部分：</h4><pre><code>堆区即为通过new的方式创建的对象（类实例）所占用的内存空间，非堆区即为代码、常量、外部访问（如文件访问流所占资源）等</code></pre><p>​    虽然java的垃圾回收机制虽然能够很好的解决内存浪费的问题，但是这种机制也仅仅的是回收堆区的资源，而对于非堆区的资源就束手无策了，针对这样的资源回收只能凭借开发人员自身的约束来解决。就算是这样（堆区有java回收机制、非堆区开发人员能够很好的解决），当运行时所需内存瞬间激增的时候JVM无奈的也要中止程序的运行。所以本文讲述的是如何解决后者的问题。<br>常见参数种类（配置内存）</p><pre><code>配置堆区：-Xms 、-Xmx、-XX:newSize、-XX:MaxnewSize、-Xmn配置非堆区：-XX:PermSize、-XX:MaxPermSize</code></pre><h5 id="2-1、堆区参数配置"><a href="#2-1、堆区参数配置" class="headerlink" title="2.1、堆区参数配置"></a>2.1、堆区参数配置</h5><ul><li>2.11、-Xms ：表示java虚拟机堆区内存初始内存分配的大小，通常为操作系统可用内存的1/64大小即可，但仍需按照实际情况进行分配。有可能真的按照这样的一个规则分配时，设计出的软件还没有能够运行得起来就挂了。</li><li><p>2.12、-Xmx： 表示java虚拟机堆区内存可被分配的最大上限，通常为操作系统可用内存的1/4大小。但是开发过程中，通常会将 -Xms 与 -Xmx两个参数的配置相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源。</p><p>  一般来讲对于堆区的内存分配只需要对上述两个参数进行合理配置即可。</p></li></ul><h5 id="2-2、非堆区参数配置"><a href="#2-2、非堆区参数配置" class="headerlink" title="2.2、非堆区参数配置"></a>2.2、非堆区参数配置</h5><ul><li>1、-XX:PermSize：表示非堆区初始内存分配大小，其缩写为permanent size（持久化内存）</li><li>2、-XX:MaxPermSize：表示对非堆区分配的内存的最大上限<ul><li>注：在配置之前一定要慎重的考虑一下自身软件所需要的非堆区内存大小，因为此处内存是不会被java垃圾回收机制进行处理的地方。并且更加要注意的是 最大堆内存与最大非堆内存的和绝对不能够超出操作系统的可用内存。</li></ul></li></ul><h4 id="3、jvm-参数配置"><a href="#3、jvm-参数配置" class="headerlink" title="3、jvm 参数配置"></a>3、jvm 参数配置</h4><ul><li><p>1、-Xms：表示java虚拟机堆区内存初始内存分配的大小，通常为操作系统可用内存的1/64大小即可，但仍需按照实际情况进行分配。</p></li><li><p>2、-Xmx：表示java虚拟机堆区内存可被分配的最大上限，通常为操作系统可用内存的1/4大小。</p><ul><li>开发过程中，通常会将-Xms 与-Xmx两个参数的配置相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源。</li></ul><p>1、-XX:newSize：表示新生代初始内存的大小，应该小于-Xms的值；<br>2、-XX:MaxnewSize：表示新生代可被分配的内存的最大上限；当然这个值应该小于-Xmx的值；<br>3、-Xmn：至于这个参数则是对 -XX:newSize、-XX:MaxnewSize两个参数的同时配置，也就是说如果通过-Xmn来配置新生代的内存大小，那么-XX:newSize = -XX:MaxnewSize　=　-Xmn，虽然会很方便，但需要注意的是这个参数是在JDK1.4版本以后才使用的。</p></li></ul><h5 id="3-1、java虚拟机对非堆区内存配置的两个参数："><a href="#3-1、java虚拟机对非堆区内存配置的两个参数：" class="headerlink" title="3.1、java虚拟机对非堆区内存配置的两个参数："></a>3.1、java虚拟机对非堆区内存配置的两个参数：</h5><p>​    1、-XX:PermSize：表示非堆区初始内存分配大小（方法区）<br>​    2、-XX:MaxPermSize：表示对非堆区分配的内存的最大上限（方法区）。</p><p>​    在配置之前一定要慎重的考虑一下自身软件所需要的非堆区内存大小，因为此处内存是不会被java垃圾回收机制进行处理的地方。并且更加要注意的是最大堆内存与最大非堆内存的和绝对不能够超出操作系统的可用内存。</p>]]></content>
      
      
      <categories>
          
          <category> jvm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA </tag>
            
            <tag> JVM </tag>
            
            <tag> GC </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux安装LAMP</title>
      <link href="/2015/01/21/Linux%E5%AE%89%E8%A3%85LAMP/"/>
      <url>/2015/01/21/Linux%E5%AE%89%E8%A3%85LAMP/</url>
      
        <content type="html"><![CDATA[<h2 id="Linux安装LAMP"><a href="#Linux安装LAMP" class="headerlink" title="Linux安装LAMP"></a>Linux安装LAMP</h2><h4 id="0、使用以下命令安装Apache："><a href="#0、使用以下命令安装Apache：" class="headerlink" title="0、使用以下命令安装Apache："></a><strong>0、使用以下命令安装Apache：</strong></h4><p>yum install httpd</p><p>安装完之后，重新启动Apache：/etc/init.d/httpd restart<br>接着将Apache设置为开机启动：chkconfig httpd on.(这一步使得服务器不需要在每次重启的时候都要手动启动httpd服务)</p><p>要查看httpd服务的启动状态，可以使用命令：chkconfig –list httpd(会显示httpd在各个级别(level)下的启动状态)</p><h4 id="1、使用以下命令安装MySQL："><a href="#1、使用以下命令安装MySQL：" class="headerlink" title="1、使用以下命令安装MySQL："></a>1、使用以下命令安装MySQL：</h4><p>yum install mysql mysql-server<br>同样，如果出现提示已安装的话，就说明系统安装了MySQL了，可以跳过这一步，否则，系统接下来会自动安装MySQL。<br>安装完成了之后，启动MySQL：/etc/init.d/mysql start</p><p>将MySQL设置为开机启动：chkconfig mysqld on<br>最后，拷贝配置文件：cp /usr/share/mysql/my-medium.cnf /etc/my.cnf (在/etc下有个my.cnf文件，直接覆盖就行了)</p><p>1.2、用以下命令给root账户设置密码</p><p>mysql_secure_installation<br>根据提示输入2次密码，就设置成功了。注意，在设置过程中，会提示删除是否anonymous用户，是否拒绝root的远程访问，是否删除测试用的数据库等，这些都需要根据自己的实际情况进行选择。最后出现：Thanks for using MySQL!，设置密码成功了。</p><p>重新启动MySQL：/etc/init.d/mysqld restart</p><ol><li>3、mysql开启远程访问权限</li></ol><p>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘myuser‘@’%’IDENTIFIED BY ‘mypassword’ WITH GRANT OPTION;</p><h4 id="2：安装PHP"><a href="#2：安装PHP" class="headerlink" title="2：安装PHP"></a><strong>2：安装PHP</strong></h4><p>2.1、使用以下命令安装PHP：</p><p>yum install php<br>根据提示往下安装就行了。安装完之后重新启动Apache：/etc/init.d/httpd restart<br>2.2、安装PHP组件，是PHP支持MySQL</p><p>可以使用命令：yum search php来查看PHP的组件，选择需要的模块进行安装：</p><p>yum install php-mysql php-gd libjpeg* php-imap php-ldap php-odbc php-pear php-xml php-xmlrpc php-mbstring php-mcrypt php-bcmath php-mhash libmcrypt</p><p>安装完之后，重启Apache：/etc/init.d/httpd restart</p><p>重启MySQL：/etc/init.d/mysqld restart</p>]]></content>
      
      
      <categories>
          
          <category> 博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> apache </tag>
            
            <tag> mysql </tag>
            
            <tag> php </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
